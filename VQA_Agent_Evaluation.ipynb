{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Agent Evaluation: Guided Run\n",
    "\n",
    "This notebook serves as the master controller for running the different evaluation pipelines defined in the `scripts/` directory. We will compare three different approaches to Visual Question Answering (VQA):\n",
    "\n",
    "1.  **Zero-Shot:** Directly asking the VLM the question.\n",
    "2.  **Classic Agent:** Enhancing the VLM's prompt with information extracted via OpenCV.\n",
    "3.  **DL Agent:** Using the VLM itself to perform Chain-of-Thought (CoT) reasoning.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”§ 1. Setup\n",
    "\n",
    "First, let's install all the required dependencies from the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¥ 2. Data Check\n",
    "\n",
    "Before we run the evaluations, let's make sure our data is loaded correctly. The following command will run the `evaluate_agents.py` script in `show_sample` mode, which loads the dataset and displays a sample image, question, and answer.\n",
    "\n",
    "> ðŸ’¡ **Note:** Make sure you have placed `vqa_dataset.csv` in the `data/` folder and all your `.png` images in the `data/images/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/evaluate_agents.py --mode show_sample --sample_index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ 3. Run Evaluation: Zero-Shot (Baseline)\n",
    "\n",
    "This is our baseline. The script will loop through all 100 samples, ask the VLM the question directly, and use the LLM-as-Judge to score the answer. This will give us the performance of the raw model.\n",
    "\n",
    "All logs will be saved to `logs/evaluation.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/evaluate_agents.py --mode zero_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– 4. Run Evaluation: Classic Agent (CV-Enhanced)\n",
    "\n",
    "Now, we run the \"Classic\" agent. For each image, this script first runs an OpenCV pipeline (`src/agent_pipelines/classic_agent.py`) to detect all objects, their colors, shapes, and coordinates. This structured data is then pre-pended to the VLM's prompt to provide it with explicit context.\n",
    "\n",
    "We expect this to improve performance, as the VLM no longer has to *find* the objects; it only has to *reason* about the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/evaluate_agents.py --mode classic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  5. Run Evaluation: DL Agent (Chain-of-Thought)\n",
    "\n",
    "Finally, we run the \"DL\" agent. This agent uses the VLM's own reasoning capabilities in a 3-step pipeline (`src/agent_pipelines/dl_agent.py`):\n",
    "\n",
    "1.  **Decompose:** Ask the VLM to create a *plan* to answer the question.\n",
    "2.  **Extract:** Ask the VLM to *describe* the image in detail.\n",
    "3.  **Synthesize:** Ask the VLM to *answer* the original question using only the plan and description.\n",
    "\n",
    "This Chain-of-Thought (CoT) approach forces the model to \"think step-by-step,\" which often leads to more accurate and robust answers for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/evaluate_agents.py --mode dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ 6. Final Analysis\n",
    "\n",
    "You can now compare the accuracy scores printed at the end of each run. For a full breakdown and analysis, please see the **'How It Works'** and **'Results & Analysis'** sections in the `README.md` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
